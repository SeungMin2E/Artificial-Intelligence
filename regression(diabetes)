1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
ğŸ“Œ ì„¤ëª…:

pandas, numpy: ë°ì´í„° ì²˜ë¦¬ ë° ìˆ˜í•™ ì—°ì‚°

seaborn, matplotlib.pyplot: ë°ì´í„° ì‹œê°í™”

sklearn.model_selection: ë°ì´í„°ì…‹ ë¶„í• (train/test)

sklearn.preprocessing: ë°ì´í„° ì „ì²˜ë¦¬(Label Encoding)

sklearn.ensemble, sklearn.tree, sklearn.linear_model, sklearn.svm: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤

sklearn.metrics: ì„±ëŠ¥ í‰ê°€
==================================================================
2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

df = pd.read_csv("C:/Users/min22/Desktop/Artifical intelligence/diabetes.csv", index_col=0)
df
ğŸ“Œ ì„¤ëª…:

diabetes.csv ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ì²« ë²ˆì§¸ ì—´(index)ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.

ë°ì´í„°ì…‹ í™•ì¸ì„ ìœ„í•´ df ì¶œë ¥.
===============================================================
3. ë°ì´í„° ì¤€ë¹„

X = df.drop("Outcome", axis=1)
X.head()

y = df["Outcome"]
y.head()
ğŸ“Œ ì„¤ëª…:

X: ë…ë¦½ ë³€ìˆ˜(íŠ¹ì§• ë°ì´í„°, Outcomeì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€).

y: ì¢…ì† ë³€ìˆ˜(ì˜ˆì¸¡ ëŒ€ìƒì¸ ë‹¹ë‡¨ë³‘ ì—¬ë¶€).

.head(): ë°ì´í„° ì¼ë¶€ ì¶œë ¥.
==================================================================
4. ë°ì´í„° ë¶„í• 

X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, shuffle=True, random_state=12)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)
ğŸ“Œ ì„¤ëª…:

train_test_split(): ë°ì´í„°ì…‹ì„ 80% (í›ˆë ¨) / 20% (í…ŒìŠ¤íŠ¸) ë¹„ìœ¨ë¡œ ë¶„í• .

shuffle=True: ë°ì´í„°ë¥¼ ì„ì–´ì„œ ë‚˜ëˆ”.

random_state=12: ëœë¤ ì‹œë“œ ê³ ì •(ì¬í˜„ì„± ë³´ì¥).

shape: ë°ì´í„° í¬ê¸° ì¶œë ¥.
===============================================================
5. ì„ í˜• íšŒê·€ (Linear Regression)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model = LinearRegression()
model.fit(X_train, y_train)
ly_preds = model.predict(X_test)

print('í‰ê· ì œê³±ê·¼ì˜¤ì°¨', mean_squared_error(ly_preds, y_test))
ğŸ“Œ ì„¤ëª…:

LinearRegression(): ì„ í˜• íšŒê·€ ëª¨ë¸ ìƒì„±.

model.fit(X_train, y_train): ëª¨ë¸ í›ˆë ¨.

ly_preds = model.predict(X_test): í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡.

mean_squared_error(): í‰ê· ì œê³±ì˜¤ì°¨(MSE) ì¶œë ¥.
=========================================================
6. MSE ì§ì ‘ ê³„ì‚°

def mse_np(actual, predicted):
    return np.mean((np.array(actual) - np.array(predicted)) ** 2)

print('í‰ê· ì œê³±ê·¼ì˜¤ì°¨', mse_np(ly_preds, y_test))

def mse(actual, predicted):
    sum_square_error = sum((a - p) ** 2 for a, p in zip(actual, predicted))
    mean_square_error = sum_square_error / len(actual)
    return mean_square_error

print('í‰ê· ì œê³±ê·¼ì˜¤ì°¨', mse(ly_preds, y_test))
ğŸ“Œ ì„¤ëª…:

mse_np(actual, predicted): numpyë¥¼ ì´ìš©í•´ MSE ê³„ì‚°.

mse(actual, predicted): forë¬¸ì„ ì´ìš©í•´ MSE ê³„ì‚°.

ë‘ ë°©ì‹ ëª¨ë‘ ê°™ì€
=========================================================
7. BMI ê¸°ì¤€ ì˜ˆì¸¡ ì‹œê°í™”

plt.figure(figsize=(10,5))
plt.scatter(X_test['BMI'], y_test, label='y_test')
plt.scatter(X_test['BMI'], ly_preds, c='y', label='ly_preds') # ë…¸ë€ìƒ‰ ì˜ˆì¸¡ê°’
plt.show()
ğŸ“Œ ì„¤ëª…:

X_test['BMI']: BMI ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ.

plt.scatter(): ì‚°ì ë„ ê·¸ë˜í”„.

y_test: ì‹¤ì œ ê°’, ly_preds: ì„ í˜• íšŒê·€ ì˜ˆì¸¡ê°’.

ë…¸ë€ìƒ‰ ì (c='y'): ì˜ˆì¸¡ê°’.
======================================================
8. ê²°ì • íŠ¸ë¦¬ íšŒê·€ (Decision Tree Regressor)

from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor()
model.fit(X_train, y_train)

dy_preds = model.predict(X_test)
print('í‰ê· ì œê³±ê·¼ì˜¤ì°¨', mean_squared_error(dy_preds, y_test))
ğŸ“Œ ì„¤ëª…:

DecisionTreeRegressor(): ê²°ì • íŠ¸ë¦¬ ê¸°ë°˜ íšŒê·€ ëª¨ë¸ ì‚¬ìš©.

dy_preds: ê²°ì • íŠ¸ë¦¬ ì˜ˆì¸¡ê°’.

MSE ì¶œë ¥.

plt.figure(figsize=(10,5))
plt.scatter(X_test['BMI'], y_test, label='y_test')
plt.scatter(X_test['BMI'], dy_preds, c='g', label='dy_preds') # ì´ˆë¡ìƒ‰ ì˜ˆì¸¡ê°’
plt.show()
ğŸ“Œ ì„¤ëª…:

BMI ê¸°ì¤€ ì˜ˆì¸¡ê°’(ì´ˆë¡ìƒ‰) ì‹œê°í™”.
=================================================================
9. ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ (Random Forest Regressor)

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)

ry_preds = model.predict(X_test)
print('í‰ê· ì œê³±ê·¼ì˜¤ì°¨', mean_squared_error(ry_preds, y_test))
ğŸ“Œ ì„¤ëª…:

RandomForestRegressor(): ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê¸°ë°˜ íšŒê·€ ëª¨ë¸ ì‚¬ìš©.

ry_preds: ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì˜ˆì¸¡ê°’.

MSE ì¶œë ¥.

plt.figure(figsize=(10,5))
plt.scatter(X_test['BMI'], y_test, label='y_test')
plt.scatter(X_test['BMI'], ry_preds, c='orange', label='ry_preds') # ì£¼í™©ìƒ‰ ì˜ˆì¸¡ê°’
plt.show()
ğŸ“Œ ì„¤ëª…:

BMI ê¸°ì¤€ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì˜ˆì¸¡ê°’(ì£¼í™©ìƒ‰) ì‹œê°í™”.
==================================================================
10. ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  íšŒê·€ (SVR)

from sklearn.svm import SVR

model = SVR(kernel='linear')
model.fit(X_train, y_train)

ry_preds = model.predict(X_test)
print('í‰ê· ì œê³±ê·¼ì˜¤ì°¨', mean_squared_error(ry_preds, y_test))
ğŸ“Œ ì„¤ëª…:

SVR(kernel='linear'): ì„ í˜• ì»¤ë„ ê¸°ë°˜ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  íšŒê·€ ëª¨ë¸ ì‚¬ìš©.

ry_preds: SVM ì˜ˆì¸¡ê°’.

MSE ì¶œë ¥.

plt.figure(figsize=(10,5))
plt.scatter(X_test['BMI'], y_test, label='y_test')
plt.scatter(X_test['BMI'], ry_preds, c='orange', label='ry_preds') # ì£¼í™©ìƒ‰ ì˜ˆì¸¡ê°’
plt.show()
ğŸ“Œ ì„¤ëª…:

BMI ê¸°ì¤€ SVM ì˜ˆì¸¡ê°’(ì£¼í™©ìƒ‰) ì‹œê°í™”.
================================================================
ğŸ§ ì½”ë“œ ìš”ì•½
âœ… ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸(Linear Regression, Decision Tree, Random Forest, SVR) ì‚¬ìš©
âœ… MSE(í‰ê· ì œê³±ì˜¤ì°¨)ë¡œ ì„±ëŠ¥ ë¹„êµ
âœ… BMI ê¸°ì¤€ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ì‹œê°í™”

ê° ëª¨ë¸ì˜ MSEë¥¼ ë¹„êµí•´ ê°€ì¥ ì¢‹ì€ íšŒê·€ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŒ!
